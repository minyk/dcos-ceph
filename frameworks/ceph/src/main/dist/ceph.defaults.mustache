# ceph.defaults
# for more config options see: https://github.com/ceph/ceph/blob/master/src/common/config_opts.h
# the format is key_path value
# the key path will be prefixed by /ceph-config/$CLUSTER

# global
/global/auth_cluster_required cephx
/global/auth_service_required cephx
/global/auth_client_required cephx
/global/max_open_files 131072
/global/mon_osd_full_ratio ".95"
/global/mon_osd_nearfull_ratio ".85"
/global/osd_pool_default_min_size 1
/global/osd_pool_default_pg_num 128
/global/osd_pool_default_pgp_num 128
/global/osd_pool_default_size 3

# auth
/auth/cephx true
/auth/cephx_cluster_require_signatures false
/auth/cephx_service_require_signatures false

# mon
/mon/mon_clock_drift_allowed ".15"
/mon/mon_clock_drift_warn_backoff 30
/mon/mon_osd_down_out_interval 600
/mon/mon_osd_min_down_reporters 4
/mon/mon_osd_report_timeout 300

# osd
/osd/osd_journal_size 100
/osd/filestore_max_sync_interval 5
/osd/filestore_merge_threshold 40
/osd/filestore_op_threads 8
/osd/filestore_split_multiple 8
/osd/osd_client_op_priority 63
/osd/osd_crush_update_on_start true
/osd/osd_max_backfills 2
/osd/osd_max_scrubs 1
/osd/osd_mon_heartbeat_interval 30
/osd/osd_objectstore "filestore"
/osd/osd_op_threads 8
/osd/osd_recovery_max_active 5
/osd/osd_recovery_max_chunk 1048576
/osd/osd_recovery_op_priority 2
/osd/osd_recovery_threads 1
/osd/pool_default_crush_rule 0

# client
/client/rbd_cache_enabled true
/client/rbd_cache_writethrough_until_flush true

# mds
/mds/mds_cache_size 100000

# mgr
/mgr/mgr_modules dashboard
